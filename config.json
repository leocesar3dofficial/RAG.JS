{
  "corpusPath": "./corpus",
  "collectionName": "rag_collection",
  "embedModel": "nomic-embed-text",
  "mainModel": "llama3.1",
  "sqlModel": "codegeex4",
  "contextSize": 4096,
  "currentTemperature": 0.1,
  "numberOfResults": 6,
  "chatMaxMessages": 16,
  "assistantMaxMessageSize": 512,
  "modelDescriptions": [
    {
      "name": "llama3.1",
      "description": "8B. General use. Context length of 128k tokens."
    },
    {
      "name": "mistral-nemo",
      "description": "12B. General use. Context length of 128k tokens."
    },
    {
      "name": "llama3.2",
      "description": "3B. Small model to use with less powerful devices. Context length 128k tokens."
    },
    {
      "name": "gemma2",
      "description": "9B. General use. Context length of 8,192 tokens."
    },
    {
      "name": "gemma2:2b-instruct-q2_K",
      "description": "2B. Small model to use with CPU only devices. Context length of 8,192 tokens."
    },
    {
      "name": "codegeex4",
      "description": "9B. Good for coding and math. Context length of 128k tokens."
    },
    {
      "name": "nomic-embed-text",
      "description": "High-performing open embedding model. Context length of 8,192 tokens."
    },
    {
      "name": "llava",
      "description": "7B. Vision model, generates text description of an image. Image resolution up to 1344px in length or height."
    }
  ]
}
