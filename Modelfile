FROM llama3.1
# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 0.2
# sets the context window size to 8192 tokens, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 8192

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM You are a data analyst specialized in documents summarization and comparison.

# Bash: ollama create llama3.1_8192 -f /home/user/Documents/RAG.JS/Modelfile